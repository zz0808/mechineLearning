#### 浅层神经网络

#### activation function
tanh(z): 特殊的，sigmoid用于 二元分类:
$$ tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} \in (-1,1)$$
对z求导为$1-tanh(z)^2$:
$$ \begin{split} tanh(z)' &= (1-\frac{2e^{-z}}{e^z+e^{-z}})' \\ &= 2\frac{e^{-z}(e^z+e^{-z})+e^{-z}(e^z-e^{-z})}{(e^z+e^{-z})^2} \\ &= 4\frac{e^{-z}e^z}{(e^z+e^{-z})^2} \\ &= \frac{(e^z+e^{-z})^2-({e^z}^2+{e^{-z}}^2-2e^ze^{-z})}{(e^z+e^{-z})^2} \\ &= 1 - tanh(z)^2\end{split}$$

relu(z): z为0时，在计算机中，是0附近的很小的邻域，理论上应该有两段，默认导数为0. 因为其导数是常数，所以相对tanh速度更快。
$$relu(z) = max(0,z)$$

leaky Relu: relu的缺点时当z为负时，其参数接下来不会被更新，因此leaky relu对z为负的情况下做了修正:
$$g(z)=max(az,z),a是很小的正数$$

#### 为什么需要非线形映射

对于恒等线形映射，无论隐藏层有多少，它最终计算的都是：
$$\hat{y}=wx+b$$
多个线性隐藏层的组合仍然是线程隐藏层，所以没有隐藏层和很多的隐藏层都能学习到相同的效果。

#### 随机初始化权重

显然，偏置b可以初始化为0，对于权重矩阵W，如果初始化为0，则对于隐藏层，其每个神经元的输出$a^{(l)}_i$都是相同的，因为显然它们在做同样的事情，由于对称性，其反向传播 $dZ^{(l)}_i$也是相同的，这样，每次更新时$w = w - dw$对于每个神经元也是相同的，所以，每一层的所有神经元都在做同样的事情，也就是每层的有多个神经元和只有一个神经元有一样的效果。所以需要用很小的随机数进行初始化，如果随机数较大，则$WX+b$会较大，对于tanh或者sigmoid等激活函数，$g(WX+b)$求导后值会很小，这样更新$w=w-dw$每次更新幅度会很小，即梯度下降慢。

